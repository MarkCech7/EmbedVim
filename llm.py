from llama_index.llms.ollama import Ollama
from llama_index.core.indices.query.query_transform import HyDEQueryTransform

def GenerateResponse(retrieved_docs, query, model_name, promptTemplate):

    if len(retrieved_docs) >= 1:
        combined_context = " ".join([doc.page_content for doc in retrieved_docs])
        prompt = promptTemplate.replace("[context_str]", combined_context).replace("[query_str]", query)
        llm = Ollama(model = model_name)
        response = llm.invoke(prompt)
 
        return response
    
def SearchWithHyDE(vectordb, query, k=3, model_name="llama3.1"):
    """
    Perform a search using both the original query and a hypothetical answer generated by the HyDE approach.

    Parameters:
    - vectordb: The vector database object.
    - query: The original user query.
    - k: Number of documents to retrieve.
    - model_name: The name of the LLM model to use for generating hypothetical answers.

    Returns:
    - retrieved_docs: A list of retrieved documents based on the similarity search.
    """
    hyde_transform = HyDEQueryTransform(llm=Ollama(model=model_name))

    if isinstance(query, str):
        transformed_query = hyde_transform(query)
    else:
        raise ValueError("Query must be a string to generate a hypothetical answer.")

    transformed_query = hyde_transform(query)
    hyde_docs = vectordb.similarity_search(transformed_query, k=k)

    original_docs = vectordb.similarity_search(query, k=k)
    combined_docs = original_docs + hyde_docs

    return combined_docs